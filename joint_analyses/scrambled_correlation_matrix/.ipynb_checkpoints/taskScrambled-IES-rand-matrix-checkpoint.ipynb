{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Scrambled Correlation Matrix\n",
    "\n",
    "### Task data within split and duration were randomly assigned to the other task (50%)  \n",
    "This matrix acts as a null distribution comparison to the actual IES correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REPLACE WITH YOUR WORKING DIRECTORY #####\n",
    "_dir = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Z-scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim_path = _dir + '/distance_discrimination/data/final_discrim.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim = pd.read_csv(final_discrim_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim.columns\n",
    "\n",
    "rmd = ['Unnamed: 0', 'workerId', 'experimentName', 'versionName',\n",
    "       'sequenceName', 'url', 'selected_row', 'windowWidth', 'windowHeight',\n",
    "       'screenWidth', 'screenHeight', 'startDate', \n",
    "       'startTime', 'trial', 'log_fixation', 'log_sceneDuration1',\n",
    "       'log_mask1', 'log_sceneDuration2', 'log_mask2', 'experimentTime',\n",
    "       'totalTime', 'age', 'gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim.drop(columns=rmd, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_accuracy_column(df):\n",
    "    \n",
    "    accuracy = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        choice = row[\"discrim_choice\"]\n",
    "        if choice == 2.0:\n",
    "            accuracy.append('missed')\n",
    "        else:    \n",
    "            depth0 = row[\"actual_depth_0\"]\n",
    "            depth1 = row[\"actual_depth_1\"]\n",
    "            if depth0 < depth1:\n",
    "                correct_choice = 0\n",
    "            if depth0 > depth1:\n",
    "                correct_choice = 1\n",
    "            if depth0 == depth1:\n",
    "                # case where depths are equal \n",
    "                correct_choice = None\n",
    "            if choice == correct_choice:\n",
    "                accuracy.append(1)\n",
    "            if type(correct_choice) == int:\n",
    "                if choice != correct_choice:\n",
    "                    accuracy.append(0)\n",
    "                \n",
    "    # Using DataFrame.insert() to add a column\n",
    "    df.insert(7, \"accuracy\", accuracy, True)            \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_discrim = add_accuracy_column(final_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subjID', 'z_scored_RT', 'cleaned_RT', 'stimulus_0', 'stimulus_1',\n",
       "       'depth_difference', 'kinect_answer', 'accuracy', 'duration',\n",
       "       'actual_depth_0', 'actual_depth_1', 'discrim_choice', 'trial_RT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "FINAL_discrim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discrim_125 = FINAL_discrim.loc[FINAL_discrim['duration'] == 125]\n",
    "final_discrim_250 = FINAL_discrim.loc[FINAL_discrim['duration'] == 250]\n",
    "final_discrim_1000 = FINAL_discrim.loc[FINAL_discrim['duration'] == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_data_125_path = _dir + '/numerical_estimate/data/z_scored_125_data.csv'\n",
    "\n",
    "VE_data_250_path = _dir + '/numerical_estimate/data/z_scored_250_data.csv'\n",
    "\n",
    "VE_data_1000_path = _dir + '/numerical_estimate/data/z_scored_1000_data.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_data_125 = pd.read_csv(VE_data_125_path)\n",
    "VE_data_250 = pd.read_csv(VE_data_250_path)\n",
    "VE_data_1000 = pd.read_csv(VE_data_1000_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = ['Unnamed: 0', 'workerId', 'experimentName', 'versionName',\n",
    "       'sequenceName', 'url', 'selected_row', 'windowWidth', 'windowHeight',\n",
    "       'screenWidth', 'screenHeight', 'startDate', 'startTime', 'trial',\n",
    "       'log_sceneDuration', 'unitSelection', 'seq_filepath', 'experimentTime',\n",
    "       'totalTime', 'age', 'gender']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_data_125.drop(columns=rm, inplace=True)\n",
    "VE_data_250.drop(columns=rm, inplace=True)\n",
    "VE_data_1000.drop(columns=rm, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stimuli(duration_df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        duration df \n",
    "    Returns:\n",
    "        all_stimuli\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(duration_df.stimulus.unique())\n",
    "\n",
    "\n",
    "def rand_data(duration_df, proportion):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        Data post outlier removal: i.e. cleaned_data \n",
    "        proportion - amount of data in each split (0.5 for 50/50 split)\n",
    "    \"\"\"\n",
    "#     start_time = process_time()\n",
    "    \n",
    "    stimuli = get_stimuli(duration_df)\n",
    "    \n",
    "    rand0_y = []\n",
    "    rand0_std = []\n",
    "    rand0_ste = []\n",
    "    rand0_stim = []\n",
    "    \n",
    "    rand0_all_ys = []\n",
    "    \n",
    "    rand0_subjIDs = []\n",
    "    \n",
    "    rand0_RTs = []\n",
    "    \n",
    "    rand1_y = []\n",
    "    rand1_std = []\n",
    "    rand1_ste = []\n",
    "    rand1_stim = []\n",
    "    \n",
    "    rand1_all_ys = []\n",
    "    \n",
    "    rand1_subjIDs = []\n",
    "    \n",
    "    rand1_RTs = []\n",
    "    \n",
    "    rand0_stim_dfs = []\n",
    "    rand1_stim_dfs = []\n",
    "\n",
    "    grouped = duration_df.groupby(duration_df.stimulus) \n",
    "    for stim in stimuli:   \n",
    "        stim_df = grouped.get_group(stim)\n",
    "        stim_df_estimates = stim_df['depth_estimate'].tolist()\n",
    "        stim_df_RTs = stim_df['trial_RT'].tolist()\n",
    "        stim_df_subjIDs = stim_df['subjID'].tolist()\n",
    "        \n",
    "        # Shuffle two lists with same order\n",
    "        # Using zip() + * operator + shuffle()\n",
    "        temp = list(zip(stim_df_subjIDs, stim_df_estimates, stim_df_RTs))\n",
    "        random.shuffle(temp)\n",
    "        random.shuffle(temp)\n",
    "        shuffled_subjIDs, shuffled_estimates, shuffled_RTs = zip(*temp)\n",
    "        # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "        shuffled_subjIDs, shuffled_estimates, shuffled_RTs = list(shuffled_subjIDs), list(shuffled_estimates), list(shuffled_RTs)\n",
    "        \n",
    "        rand0_all_ys.append(np.array(shuffled_estimates[0:int(len(shuffled_estimates)*proportion)]))\n",
    "        rand0_subjIDs.append(np.array(shuffled_subjIDs[0:int(len(shuffled_subjIDs)*proportion)]))\n",
    "        rand0_RTs.append(np.array(shuffled_RTs[0:int(len(shuffled_RTs)*proportion)]))\n",
    "        \n",
    "        estim_avg_df0 = np.mean(np.array(shuffled_estimates[0:int(len(shuffled_estimates)*proportion)]))\n",
    "        estim_std0 = np.std(np.array(shuffled_estimates[0:int(len(shuffled_estimates)*proportion)]))\n",
    "        estim_ste0 = stats.sem(np.array(shuffled_estimates[0:int(len(shuffled_estimates)*proportion)]))\n",
    "        \n",
    "        rand0_y.append(estim_avg_df0) \n",
    "        rand0_std.append(estim_std0)\n",
    "        rand0_ste.append(estim_ste0)\n",
    "        rand0_stim.append(stim)\n",
    "        \n",
    "        # subjects included in split 0 for a particular stimulus\n",
    "        stim_rand0_subjIDs = np.array(shuffled_subjIDs[0:int(len(shuffled_subjIDs)*proportion)])\n",
    "        # keep data from subjects that are in the above list \n",
    "        stim_rand0_df = stim_df.loc[stim_df['subjID'].isin(stim_rand0_subjIDs)]\n",
    "        rand0_stim_dfs.append(stim_rand0_df)\n",
    "        \n",
    "#         print(len(stim_df.subjID.unique()), len(stim_rand0_df.subjID.unique()))\n",
    "        \n",
    "        \n",
    "        rand1_all_ys.append(np.array(shuffled_estimates[int(len(shuffled_estimates)*proportion):]))\n",
    "        rand1_subjIDs.append(np.array(shuffled_subjIDs[int(len(shuffled_subjIDs)*proportion):]))\n",
    "        rand1_RTs.append(np.array(shuffled_RTs[int(len(shuffled_RTs)*proportion):]))\n",
    "        \n",
    "        estim_avg_df1 = np.mean(np.array(shuffled_estimates[int(len(shuffled_estimates)*proportion):]))\n",
    "        estim_std1 = np.std(np.array(shuffled_estimates[int(len(shuffled_estimates)*proportion):]))\n",
    "        estim_ste1 = stats.sem(np.array(shuffled_estimates[int(len(shuffled_estimates)*proportion):]))\n",
    "        \n",
    "        rand1_y.append(estim_avg_df1) \n",
    "        rand1_std.append(estim_std1)\n",
    "        rand1_ste.append(estim_ste1)\n",
    "        rand1_stim.append(stim)\n",
    "        \n",
    "        # subjects included in split 1 for a particular stimulus\n",
    "        stim_rand1_subjIDs = np.array(shuffled_subjIDs[int(len(shuffled_subjIDs)*proportion):])\n",
    "        # keep data from subjects that are in the above list        \n",
    "        stim_rand1_df = stim_df.loc[stim_df['subjID'].isin(stim_rand1_subjIDs)]\n",
    "        rand1_stim_dfs.append(stim_rand1_df)\n",
    "\n",
    "        \n",
    "    # combine all stim dataframes for split 0\n",
    "    rand0_df = pd.concat(rand0_stim_dfs)\n",
    "    \n",
    "    rand0_y = np.array(rand0_y).reshape(1,-1)[0]\n",
    "    rand0_std = np.array(rand0_std).reshape(1,-1)[0]\n",
    "    rand0_ste = np.array(rand0_ste).reshape(1,-1)[0]\n",
    "    rand0_stim = np.array(rand0_stim).reshape(1,-1)[0]\n",
    "    \n",
    "    # combine all stim dataframes for split 1\n",
    "    rand1_df = pd.concat(rand1_stim_dfs)\n",
    "\n",
    "    rand1_y = np.array(rand1_y).reshape(1,-1)[0]\n",
    "    rand1_std = np.array(rand1_std).reshape(1,-1)[0]\n",
    "    rand1_ste = np.array(rand1_ste).reshape(1,-1)[0]\n",
    "    rand1_stim = np.array(rand1_stim).reshape(1,-1)[0]\n",
    "    \n",
    "    \n",
    "#     end_time = process_time()\n",
    "#     print(f\"rand_data: {(end_time - start_time)} seconds\")\n",
    "    return [rand0_y, rand0_std, rand0_ste, rand0_stim], rand0_df, [rand1_y, rand1_std, rand1_ste, rand1_stim], rand1_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0_125, r0_125_df, r1_125, r1_125_df = rand_data(VE_data_125, 0.5)\n",
    "r0_250, r0_250_df, r1_250, r1_250_df = rand_data(VE_data_250, 0.5)\n",
    "r0_1000, r0_1000_df, r1_1000, r1_1000_df = rand_data(VE_data_1000, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(df):\n",
    "    \"\"\"\n",
    "    * ACCURACY BASED ON THE KINECT\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    acc = df['accuracy']\n",
    "    \n",
    "\n",
    "    missed = [a for a in list(df['accuracy']) if type(a) != int]\n",
    "    accuracy = [a for a in list(df['accuracy']) if type(a) == int]\n",
    "    \n",
    "    \n",
    "    # count correct/count total \n",
    "    prop_correct = sum(accuracy)/len(accuracy)\n",
    "    \n",
    "    # proportion of correct answers, count correct, count total, count missed \n",
    "    return prop_correct, sum(accuracy), len(accuracy), len(missed)\n",
    "\n",
    "def get_RT(df):\n",
    "    \"\"\"    \n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        array of RTs, avg RT, std, ste \n",
    "    \"\"\"\n",
    "    \n",
    "    list_RTs = np.array(df['trial_RT'].tolist())\n",
    "    \n",
    "    return list_RTs, np.mean(list_RTs) ,np.std(list_RTs), stats.sem(list_RTs)\n",
    "\n",
    "\n",
    "def get_zscoredRT(df):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        array of zscored RTs, avg RT and std   \n",
    "    \"\"\"\n",
    "    \n",
    "    list_RTs = np.array(df['z_scored_RT'].tolist())\n",
    "    \n",
    "    return list_RTs, np.mean(list_RTs) ,np.std(list_RTs), stats.sem(list_RTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_discrimination_stats(df, proportion):\n",
    "    '''\n",
    "    Individual discrimination performance and RT \n",
    "    '''\n",
    "    \n",
    "#     start_time = process_time()\n",
    "\n",
    "    all_stim0 = [elem for elem in df.stimulus_0.unique() if type(elem) == str ]\n",
    "\n",
    "    stimuli_stats_split0 = {}\n",
    "    stimuli_stats_split1 = {}\n",
    "    \n",
    "    split0 = []\n",
    "    split1 = []\n",
    "    for stim0 in all_stim0:\n",
    "        stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "        other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "        stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "        # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "        stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "        stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "        stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "        stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "                \n",
    "        stim_depthdiff = stim_df['depth_difference'][0]\n",
    "        \n",
    "        stim0_depth = stim_df['actual_depth_0'][0]\n",
    "        stim1_depth = stim_df['actual_depth_1'][0]\n",
    "        stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "        \n",
    "        kinect_answer = stim0_df.kinect_answer.unique()[0]\n",
    "        \n",
    "        #### SPLIT DATA INTO HALF AND CREATE STIMULI STATS PER HALF PER DURATION ####\n",
    "\n",
    "        # Creating a dataframe with 50% values of original dataframe\n",
    "        stim_125_df_split0 = stim_125_df.sample(frac = proportion, replace=False)\n",
    "        split0.append(stim_125_df_split0)\n",
    "        # Creating dataframe with rest of the 50% values\n",
    "        stim_125_df_split1 = stim_125_df.drop(stim_125_df_split0.index)\n",
    "        split1.append(stim_125_df_split1)\n",
    "        \n",
    "        # Creating a dataframe with 50% values of original dataframe\n",
    "        stim_250_df_split0 = stim_250_df.sample(frac = proportion, replace=False)\n",
    "        split0.append(stim_250_df_split0)\n",
    "        # Creating dataframe with rest of the 50% values\n",
    "        stim_250_df_split1 = stim_250_df.drop(stim_250_df_split0.index)\n",
    "        split1.append(stim_250_df_split1)\n",
    "        \n",
    "        # Creating a dataframe with 50% values of original dataframe\n",
    "        stim_1000_df_split0 = stim_1000_df.sample(frac = proportion, replace=False)\n",
    "        split0.append(stim_1000_df_split0)\n",
    "        # Creating dataframe with rest of the 50% values\n",
    "        stim_1000_df_split1 = stim_1000_df.drop(stim_1000_df_split0.index)\n",
    "        split1.append(stim_1000_df_split1)\n",
    "        \n",
    "        \n",
    "        # SPLIT 0\n",
    "        stim_acc_125_s0 = get_accuracy(stim_125_df_split0)\n",
    "        stim_acc_250_s0 = get_accuracy(stim_250_df_split0)\n",
    "        stim_acc_1000_s0 = get_accuracy(stim_1000_df_split0)\n",
    "\n",
    "        stim_RT_125_s0 = get_RT(stim_125_df_split0)\n",
    "        stim_RT_250_s0 = get_RT(stim_250_df_split0)\n",
    "        stim_RT_1000_s0 = get_RT(stim_1000_df_split0)\n",
    "        \n",
    "        stim_zsRT_125_s0 = get_zscoredRT(stim_125_df_split0)\n",
    "        stim_zsRT_250_s0 = get_zscoredRT(stim_250_df_split0)\n",
    "        stim_zsRT_1000_s0 = get_zscoredRT(stim_1000_df_split0)\n",
    "        \n",
    "        stimuli_stats_split0[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125_s0,\n",
    "                                    'accuracy_250': stim_acc_250_s0,\n",
    "                                    'accuracy_1000': stim_acc_1000_s0,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'depthdifference': stim_depthdiff, \n",
    "                                    'RT_125': stim_RT_125_s0,\n",
    "                                    'RT_250': stim_RT_250_s0,\n",
    "                                    'RT_1000': stim_RT_1000_s0,\n",
    "                                    'zsRT_125': stim_zsRT_125_s0,\n",
    "                                    'zsRT_250': stim_zsRT_250_s0,\n",
    "                                    'zsRT_1000': stim_zsRT_1000_s0,\n",
    "                                    'kinect_answer': kinect_answer}\n",
    "        \n",
    "        # SPLIT 1\n",
    "        stim_acc_125_s1 = get_accuracy(stim_125_df_split1)\n",
    "        stim_acc_250_s1 = get_accuracy(stim_250_df_split1)\n",
    "        stim_acc_1000_s1 = get_accuracy(stim_1000_df_split1)\n",
    "\n",
    "        stim_RT_125_s1 = get_RT(stim_125_df_split1)\n",
    "        stim_RT_250_s1 = get_RT(stim_250_df_split1)\n",
    "        stim_RT_1000_s1 = get_RT(stim_1000_df_split1)\n",
    "        \n",
    "        stim_zsRT_125_s1 = get_zscoredRT(stim_125_df_split0)\n",
    "        stim_zsRT_250_s1 = get_zscoredRT(stim_250_df_split0)\n",
    "        stim_zsRT_1000_s1 = get_zscoredRT(stim_1000_df_split0)\n",
    "\n",
    "        stimuli_stats_split1[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125_s1,\n",
    "                                    'accuracy_250': stim_acc_250_s1,\n",
    "                                    'accuracy_1000': stim_acc_1000_s1,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'depthdifference': stim_depthdiff, \n",
    "                                    'RT_125': stim_RT_125_s1,\n",
    "                                    'RT_250': stim_RT_250_s1,\n",
    "                                    'RT_1000': stim_RT_1000_s1,\n",
    "                                    'zsRT_125': stim_zsRT_125_s1,\n",
    "                                    'zsRT_250': stim_zsRT_250_s1,\n",
    "                                    'zsRT_1000': stim_zsRT_1000_s1,\n",
    "                                    'kinect_answer': kinect_answer}\n",
    "        \n",
    "    end_time = process_time()\n",
    "#     print(f\"rand_data: {(end_time - start_time)} seconds\")\n",
    "    \n",
    "    return stimuli_stats_split0, pd.concat(split0), stimuli_stats_split1, pd.concat(split1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_discrim_performance_s0, final_discrim_s0, all_discrim_performance_s1, final_discrim_s1 = individual_discrimination_stats(FINAL_discrim, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answerkey(discrim_performance, VE_data):\n",
    "    '''\n",
    "    Generates discrimination trial answer key based off of verbal judgement data\n",
    "    '''\n",
    "    answerkey = {}\n",
    "    \n",
    "    final_y = VE_data[0]\n",
    "    final_std = VE_data[1]\n",
    "    final_ste = VE_data[2]\n",
    "    final_stim = VE_data[3]\n",
    "\n",
    "    for key in discrim_performance.keys():\n",
    "        targetimg0 = key.split('/')[-1]\n",
    "        folder0 = targetimg0[:-11]\n",
    "        depth_dur_path0 = 'depth_duration_stimuli/' + folder0 + '/' + targetimg0\n",
    "        idx0 = np.where(final_stim == depth_dur_path0)[0][0]\n",
    "        avg_estim_stim0 = final_y[idx0]\n",
    "        std0 = final_std[idx0]\n",
    "        ste0 = final_ste[idx0]\n",
    "    \n",
    "        targetimg1 = discrim_performance[key]['stimulus_1'].split('/')[-1]\n",
    "        folder1 = targetimg1[:-11]\n",
    "        depth_dur_path1 = 'depth_duration_stimuli/' + folder1 + '/' + targetimg1\n",
    "        idx1= np.where(final_stim == depth_dur_path1)[0][0]\n",
    "        avg_estim_stim1 = final_y[idx1]\n",
    "        std1 = final_std[idx1]\n",
    "        ste1 = final_ste[idx1]\n",
    "    \n",
    "        kinect_answer = discrim_performance[key]['kinect_answer'].split('/')[-1]\n",
    "\n",
    "        if avg_estim_stim0 < avg_estim_stim1:\n",
    "            # Which target is CLOSER to you?\n",
    "            answer = targetimg0\n",
    "        if avg_estim_stim0 == avg_estim_stim1:\n",
    "            print(targetimg0, targetimg1)\n",
    "        if avg_estim_stim0 > avg_estim_stim1:\n",
    "            answer = targetimg1\n",
    "\n",
    "        answerkey[key] = {'stimulus_1': targetimg1,\n",
    "                                   'stimulus_0_avg_estim': avg_estim_stim0,\n",
    "                                   'stimulus_1_avg_estim': avg_estim_stim1,\n",
    "                                   'answer': answer,\n",
    "                                   'std0': std0,\n",
    "                                   'std1': std1,\n",
    "                                   'kinect_answer': kinect_answer}\n",
    "        \n",
    "    return answerkey\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak0_125 = get_answerkey(all_discrim_performance_s0, r0_125)\n",
    "ak0_250 = get_answerkey(all_discrim_performance_s0, r0_250)\n",
    "ak0_1000 = get_answerkey(all_discrim_performance_s0, r0_1000)\n",
    "\n",
    "ak1_125 = get_answerkey(all_discrim_performance_s1, r1_125)\n",
    "ak1_250 = get_answerkey(all_discrim_performance_s1, r1_250)\n",
    "ak1_1000 = get_answerkey(all_discrim_performance_s1, r1_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VE Coded Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VE_accuracy(stim0, df, answerkey):\n",
    "    '''\n",
    "    Accuracy based on the verbal judgement data \n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    args:\n",
    "        df  \n",
    "    returns:\n",
    "        proportion of correct responses, count of correct responses, count of total trials  \n",
    "    \"\"\"\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    count_total = 0\n",
    "    count_missed = 0\n",
    "    \n",
    "    VE_correct_answer = answerkey[stim0]['answer']\n",
    "    kinect_correct_answer = answerkey[stim0]['kinect_answer']\n",
    "    \n",
    "    for idx, row in df.iterrows(): \n",
    "        choice = row[\"discrim_choice\"]\n",
    "        count_total += 1\n",
    "        if choice == 0.0: image_choice = row[\"stimulus_0\"]\n",
    "                \n",
    "        if choice == 1.0: image_choice = row[\"stimulus_1\"]\n",
    "                \n",
    "        if choice == 2.0: count_missed += 1\n",
    "            \n",
    "        if choice == 3.0: count_missed += 1\n",
    "        \n",
    "        try:\n",
    "            if image_choice.split('/')[-1] == VE_correct_answer: count_correct += 1\n",
    "        except: pass\n",
    "        \n",
    "#     standardError = (0.5*(1-0.5))/count_total\n",
    "    p = count_correct/count_total\n",
    "    standardError = np.sqrt((p*(1-p))/count_total)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if VE_correct_answer == kinect_correct_answer:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'pos'\n",
    "    else:\n",
    "        return count_correct/count_total, count_correct, count_total, count_missed, standardError, 'neg'\n",
    "\n",
    "    \n",
    "def main_VE_coded_discrim_accuracy(df, answerkey_125, answerkey_250, answerkey_1000):\n",
    "    '''\n",
    "    Args:\n",
    "        df: final discrimination dataframe\n",
    "        answerkey_250 & answerkey_1000: VE coded answer key\n",
    "    '''\n",
    "#     start_time = process_time()\n",
    "    \n",
    "    all_stim0 = df.stimulus_0.unique()\n",
    "    \n",
    "    stimuli_stats = {}\n",
    "    for stim0 in all_stim0:\n",
    "        # dataframe for stimulus 0\n",
    "        stim0_df = df.loc[df['stimulus_0'] == stim0]\n",
    "        # name of stimulus 1\n",
    "        other_stim = stim0_df.stimulus_1.unique()[0]\n",
    "        # dataframe where stimulus 0 is presented SECOND (same trial)\n",
    "        stim1_df = df.loc[df['stimulus_1'] == stim0]\n",
    "\n",
    "        # df for a specific discrimination trial (collapsed on stim presentation order)\n",
    "        stim_df = pd.concat([stim0_df, stim1_df], ignore_index=True)\n",
    "        stim_125_df = stim_df[stim_df['duration'] == 125.0]\n",
    "        stim_250_df = stim_df[stim_df['duration'] == 250.0]\n",
    "        stim_1000_df = stim_df[stim_df['duration'] == 1000.0] \n",
    "\n",
    "        stim0_depth = stim_df['actual_depth_0'][0]\n",
    "        stim1_depth = stim_df['actual_depth_1'][0]\n",
    "        stim_depthbin = np.mean(np.array([stim0_depth,stim1_depth]))\n",
    "\n",
    "        stim_acc_125 = VE_accuracy(stim0, stim_125_df, answerkey_125)\n",
    "        stim_acc_250 = VE_accuracy(stim0, stim_250_df, answerkey_250)\n",
    "        stim_acc_1000 = VE_accuracy(stim0, stim_1000_df, answerkey_1000)\n",
    "\n",
    "        stim_RT_125 = get_RT(stim_125_df)\n",
    "        stim_RT_250 = get_RT(stim_250_df)\n",
    "        stim_RT_1000 = get_RT(stim_1000_df)\n",
    "\n",
    "        # difference between verbal judgements divided by joint variance \n",
    "        # abs(VE1-VE2)/sqrt(stda^2 + std2^2)\n",
    "            \n",
    "        std0_125 = answerkey_125[stim0]['std0']\n",
    "        std1_125 = answerkey_125[stim0]['std1']\n",
    "        joint_variance_125 = np.sqrt(std0_125**2 + std1_125**2)\n",
    "        JV_regressor_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])/joint_variance_125\n",
    "\n",
    "        std0_250 = answerkey_250[stim0]['std0']\n",
    "        std1_250 = answerkey_250[stim0]['std1']\n",
    "        joint_variance_250 = np.sqrt(std0_250**2 + std1_250**2)\n",
    "        JV_regressor_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])/joint_variance_250\n",
    "\n",
    "        std0_1000 = answerkey_1000[stim0]['std0']\n",
    "        std1_1000 = answerkey_1000[stim0]['std1']\n",
    "        joint_variance_1000 = np.sqrt(std0_1000**2 + std1_1000**2)\n",
    "        JV_regressor_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])/joint_variance_1000\n",
    "            \n",
    "        if stim_acc_125[-1] == 'pos':\n",
    "            VE_depthdifference_125 = abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim'])\n",
    "        else:\n",
    "            VE_depthdifference_125 = -(abs(answerkey_125[stim0]['stimulus_0_avg_estim'] - answerkey_125[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "        if stim_acc_250[-1] == 'pos':\n",
    "            VE_depthdifference_250 = abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim'])\n",
    "        else:\n",
    "            VE_depthdifference_250 = -(abs(answerkey_250[stim0]['stimulus_0_avg_estim'] - answerkey_250[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "        if stim_acc_1000[-1] == 'pos':\n",
    "            VE_depthdifference_1000 = abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim'])\n",
    "        else:\n",
    "            VE_depthdifference_1000 = -(abs(answerkey_1000[stim0]['stimulus_0_avg_estim'] - answerkey_1000[stim0]['stimulus_1_avg_estim']))\n",
    "            \n",
    "        stimuli_stats[stim0] = {'stimulus_1': other_stim,\n",
    "                                    'accuracy_125': stim_acc_125,\n",
    "                                    'accuracy_250': stim_acc_250,\n",
    "                                    'accuracy_1000': stim_acc_1000,\n",
    "                                    'avg_depth': stim_depthbin,\n",
    "                                    'VE_depthdifference_125': VE_depthdifference_125,\n",
    "                                    'VE_depthdifference_250': VE_depthdifference_250, \n",
    "                                    'VE_depthdifference_1000': VE_depthdifference_1000,\n",
    "                                    'RT_125': stim_RT_125,\n",
    "                                    'RT_250': stim_RT_250,\n",
    "                                    'RT_1000': stim_RT_1000,\n",
    "                                    'zsRT_125': get_zscoredRT(stim_125_df),\n",
    "                                    'zsRT_250': get_zscoredRT(stim_250_df),\n",
    "                                    'zsRT_1000': get_zscoredRT(stim_1000_df),\n",
    "                                    'JV_regressor_125': JV_regressor_125,\n",
    "                                    'JV_regressor_250': JV_regressor_250,\n",
    "                                    'JV_regressor_1000': JV_regressor_1000}\n",
    "\n",
    "\n",
    "#     end_time = process_time()\n",
    "#     print(f\"main_VE_coded_discrim_accuracy: {(end_time - start_time)} seconds\")\n",
    "    \n",
    "    return stimuli_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_coded_discrim_performance0 = main_VE_coded_discrim_accuracy(final_discrim_s0, ak0_125, ak0_250, ak0_1000)\n",
    "VE_coded_discrim_performance1 = main_VE_coded_discrim_accuracy(final_discrim_s1, ak1_125, ak1_250, ak1_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrimination_results(discrim_performance, duration):\n",
    "    '''\n",
    "    Returns lists of discrimination data results\n",
    "    '''\n",
    "    if duration == 125:\n",
    "        n_VE_estim_diff = [discrim_performance[elem]['VE_depthdifference_125'] for elem in discrim_performance]\n",
    "        n_VE_accuracy = [discrim_performance[elem]['accuracy_125'][0] for elem in discrim_performance]\n",
    "        n_VE_ste = [discrim_performance[elem]['accuracy_125'][-2] for elem in discrim_performance]\n",
    "        n_avg_RT = [discrim_performance[elem]['RT_125'][1] for elem in discrim_performance]\n",
    "        n_avg_RT_ste = [discrim_performance[elem]['RT_125'][-1] for elem in discrim_performance]\n",
    "        n_avg_zsRT = [discrim_performance[elem]['zsRT_125'][1] for elem in discrim_performance]\n",
    "        n_avg_zsRT_ste = [discrim_performance[elem]['zsRT_125'][-1] for elem in discrim_performance]\n",
    "        n_JV = [discrim_performance[elem]['JV_regressor_125'] for elem in discrim_performance]\n",
    "        \n",
    "    if duration == 250:\n",
    "        n_VE_estim_diff = [discrim_performance[elem]['VE_depthdifference_250'] for elem in discrim_performance]\n",
    "        n_VE_accuracy = [discrim_performance[elem]['accuracy_250'][0] for elem in discrim_performance]\n",
    "        n_VE_ste = [discrim_performance[elem]['accuracy_250'][-2] for elem in discrim_performance]\n",
    "        n_avg_RT = [discrim_performance[elem]['RT_250'][1] for elem in discrim_performance]\n",
    "        n_avg_RT_ste = [discrim_performance[elem]['RT_250'][-1] for elem in discrim_performance]\n",
    "        n_avg_zsRT = [discrim_performance[elem]['zsRT_250'][1] for elem in discrim_performance]\n",
    "        n_avg_zsRT_ste = [discrim_performance[elem]['zsRT_250'][-1] for elem in discrim_performance]\n",
    "        n_JV = [discrim_performance[elem]['JV_regressor_250'] for elem in discrim_performance]\n",
    "        \n",
    "    if duration == 1000:\n",
    "        n_VE_estim_diff = [discrim_performance[elem]['VE_depthdifference_1000'] for elem in discrim_performance]\n",
    "        n_VE_accuracy = [discrim_performance[elem]['accuracy_1000'][0] for elem in discrim_performance]\n",
    "        n_VE_ste = [discrim_performance[elem]['accuracy_1000'][-2] for elem in discrim_performance]\n",
    "        n_avg_RT = [discrim_performance[elem]['RT_1000'][1] for elem in discrim_performance]\n",
    "        n_avg_RT_ste = [discrim_performance[elem]['RT_1000'][-1] for elem in discrim_performance]\n",
    "        n_avg_zsRT = [discrim_performance[elem]['zsRT_1000'][1] for elem in discrim_performance]\n",
    "        n_avg_zsRT_ste = [discrim_performance[elem]['zsRT_1000'][-1] for elem in discrim_performance]\n",
    "        n_JV = [discrim_performance[elem]['JV_regressor_1000'] for elem in discrim_performance]\n",
    "\n",
    "    n_stim = [elem for elem in discrim_performance]      \n",
    "    \n",
    "    return n_VE_estim_diff, n_VE_accuracy, n_VE_ste, n_avg_RT, n_avg_RT_ste, n_JV, n_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination_results0_125 = get_discrimination_results(VE_coded_discrim_performance0, 125)\n",
    "discrimination_results0_250 = get_discrimination_results(VE_coded_discrim_performance0, 250)\n",
    "discrimination_results0_1000 = get_discrimination_results(VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "discrimination_results1_125 = get_discrimination_results(VE_coded_discrim_performance1, 125)\n",
    "discrimination_results1_250 = get_discrimination_results(VE_coded_discrim_performance1, 250)\n",
    "discrimination_results1_1000 = get_discrimination_results(VE_coded_discrim_performance1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-new-column-based-on-other-columns-pandas-5586d87de73d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Performance for VE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VE_performance(stimuli, duration_df, discrimination_performance, duration_answerkey):\n",
    "    '''\n",
    "    Args:\n",
    "        stimuli:  depth discrimination stimuli list \n",
    "        duration_df: filtered VE duration dataframe \n",
    "        discrimination performance: dictionary of discrim performance \n",
    "    '''\n",
    "#     start_time = process_time()\n",
    "\n",
    "    VE_Performance = {}\n",
    "            \n",
    "    for im0 in stimuli:\n",
    "        performance = []\n",
    "        # loop through all participants\n",
    "        for subjID in duration_df.subjID.unique():\n",
    "            # filter to just the subjects data df\n",
    "            subjdf = duration_df.loc[duration_df['subjID'] == subjID]\n",
    "            \n",
    "            try:\n",
    "                im0_VE = 'depth_duration_stimuli/' + im0.split('/')[1] + '/' + im0.split('/')[2]\n",
    "                im0_row = subjdf.loc[subjdf['stimulus'] == im0_VE]\n",
    "                im0_estimate = im0_row['depth_estimate'].tolist()[0]\n",
    "\n",
    "                im1 = discrimination_performance[im0]['stimulus_1'][29:]\n",
    "                im1_VE = 'depth_duration_stimuli/' + im1\n",
    "                im1_row = subjdf.loc[subjdf['stimulus'] == im1_VE]\n",
    "                im1_estimate = im1_row['depth_estimate'].tolist()[0]\n",
    "                \n",
    "\n",
    "                if im0_estimate < im1_estimate:\n",
    "                    p_ans = im0.split('/')[-1]\n",
    "                else:\n",
    "                    p_ans = im1.split('/')[-1]\n",
    "                try:\n",
    "                    answerkey_answer = duration_answerkey[im0]['answer']\n",
    "                except:\n",
    "                    answerkey_answer = duration_answerkey['depth_discrimination_stimuli/' + im1]['answer']\n",
    "                if p_ans == answerkey_answer:\n",
    "                    trial_acc = 0 # CORRECT\n",
    "                    performance.append(trial_acc)\n",
    "                else:\n",
    "                    trial_acc = 1 # INCORRECT\n",
    "                    performance.append(trial_acc)\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "        VE_Performance[im0] = performance\n",
    "        \n",
    "#     print(f\"main_VE_coded_discrim_accuracy: {(process_time() - start_time)} seconds\")\n",
    "\n",
    "\n",
    "    VE_PC = {}\n",
    "\n",
    "    for key in VE_Performance:\n",
    "        performance = VE_Performance[key]\n",
    "        correct_count = performance.count(0)\n",
    "        incorrect_count = performance.count(1)\n",
    "        total = len(performance)\n",
    "        pc = correct_count/total\n",
    "        VE_PC[key] = pc\n",
    "        \n",
    "    \n",
    "    \n",
    "    return VE_PC\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE_performance0_125 = VE_performance(discrimination_results0_125[-1], r0_125_df, VE_coded_discrim_performance0, ak0_125)\n",
    "VE_performance0_250 = VE_performance(discrimination_results0_250[-1], r0_250_df, VE_coded_discrim_performance0, ak0_250)\n",
    "VE_performance0_1000 = VE_performance(discrimination_results0_1000[-1], r0_1000_df, VE_coded_discrim_performance0, ak0_1000)\n",
    "\n",
    "VE_performance1_125 = VE_performance(discrimination_results1_125[-1], r1_125_df, VE_coded_discrim_performance1, ak1_125)\n",
    "VE_performance1_250 = VE_performance(discrimination_results1_250[-1], r1_250_df, VE_coded_discrim_performance1, ak1_250)\n",
    "VE_performance1_1000 = VE_performance(discrimination_results1_1000[-1], r1_1000_df, VE_coded_discrim_performance1, ak1_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matched Discrimination Performance (Accuracy and RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matched_performance(VE_Performance, discrimination_performance, duration):\n",
    "    '''\n",
    "    Args:\n",
    "        VE_performance: dict of VE performance for a specific duration\n",
    "        discrimination_performance: complete discrim performance\n",
    "        duration: int, duration value\n",
    "    Returns:\n",
    "        Trial matched VE and Discrimination performance\n",
    "        List of stimuli \n",
    "        \n",
    "    '''\n",
    "    Discrim_VEmatched_PC = {}\n",
    "    all_stim = []\n",
    "    all_VE_PC = []\n",
    "    all_Discrim_VEmatched_PC = []\n",
    "\n",
    "    for key in VE_Performance:\n",
    "        \n",
    "        im_VE_PC = VE_Performance[key]\n",
    "        if duration == 125:\n",
    "            im_Discrim_PC = discrimination_performance[key]['accuracy_125'][0]\n",
    "        if duration == 250:\n",
    "            im_Discrim_PC = discrimination_performance[key]['accuracy_250'][0]\n",
    "        if duration == 1000:\n",
    "            im_Discrim_PC = discrimination_performance[key]['accuracy_1000'][0]\n",
    "        Discrim_VEmatched_PC[key] = [im_VE_PC, im_Discrim_PC]\n",
    "\n",
    "        all_stim.append(key)\n",
    "        all_VE_PC.append(im_VE_PC)\n",
    "        all_Discrim_VEmatched_PC.append(im_Discrim_PC)\n",
    "        \n",
    "    return all_Discrim_VEmatched_PC, all_VE_PC, all_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_discrim_performance0_125, m_VE_performance0_125, m_stim0_125 = matched_performance(VE_performance0_125, VE_coded_discrim_performance0, 125)\n",
    "m_discrim_performance0_250, m_VE_performance0_250, m_stim0_250 = matched_performance(VE_performance0_250, VE_coded_discrim_performance0, 250)\n",
    "m_discrim_performance0_1000, m_VE_performance0_1000, m_stim0_1000 = matched_performance(VE_performance0_1000, VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "m_discrim_performance1_125, m_VE_performance1_125, m_stim1_125 = matched_performance(VE_performance1_125, VE_coded_discrim_performance1, 125)\n",
    "m_discrim_performance1_250, m_VE_performance1_250, m_stim1_250 = matched_performance(VE_performance1_250, VE_coded_discrim_performance1, 250)\n",
    "m_discrim_performance1_1000, m_VE_performance1_1000, m_stim1_1000 = matched_performance(VE_performance1_1000, VE_coded_discrim_performance1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RT_performance(duration_df, discrimination_performance, duration, zscoredRT = True):\n",
    "    '''\n",
    "    Args:\n",
    "        duration_df: filtered VE duration dataframe\n",
    "        discrimination_performance: dict of discrimination performance\n",
    "        duration: int value for duration \n",
    "    ReturnsL\n",
    "    arr_discrim_RT: array of RTs for discrimination trials\n",
    "    arr_discrim_RT_ste: array of ste of RT for each discrimination trial\n",
    "    arr_VE_RT: array of RTs for VE trials\n",
    "    stim_RT: list of stimuli that is matched with arr_discrim_RT and arr_VE_RT \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    arr_discrim_RT = []\n",
    "    arr_discrim_RT_ste = []\n",
    "    arr_VE_RT = []\n",
    "    stim_RT = []\n",
    "\n",
    "#     start_time = process_time()\n",
    "    for key in discrimination_performance:\n",
    "        stim_RT.append(key)\n",
    "        im0 = key.split('/')[1]\n",
    "        im1 = discrimination_performance[key]['stimulus_1'].split('/')[1]\n",
    "        \n",
    "        if zscoredRT == True:\n",
    "            dur_key = 'zsRT_' + str(duration)\n",
    "        else:\n",
    "            dur_key = 'RT_' + str(duration)\n",
    "        dtrial_RT = discrimination_performance[key][dur_key][1]\n",
    "        dtrial_RT_ste = discrimination_performance[key][dur_key][-1]\n",
    "\n",
    "            \n",
    "        arr_discrim_RT.append(dtrial_RT)\n",
    "        arr_discrim_RT_ste.append(dtrial_RT_ste)\n",
    "        \n",
    "        \n",
    "        RTs = []\n",
    "        # loop through all participants\n",
    "        for subjID in duration_df.subjID.unique():\n",
    "            # filter to just the subjects data df\n",
    "            subjdf = duration_df.loc[duration_df['subjID'] == subjID]\n",
    "            \n",
    "            im0_VE = 'depth_duration_stimuli/' + key.split('/')[1] + '/' + key.split('/')[2]\n",
    "            im0_row = subjdf.loc[subjdf['stimulus'] == im0_VE]\n",
    "            if len(im0_row) > 0:\n",
    "                im0_RT = im0_row['trial_RT'].tolist()[0]\n",
    "\n",
    "                im1 = discrimination_performance[key]['stimulus_1'][29:]\n",
    "                im1_VE = 'depth_duration_stimuli/' + im1\n",
    "\n",
    "                im1_row = subjdf.loc[subjdf['stimulus'] == im1_VE]\n",
    "                if len(im1_row) > 0:\n",
    "                    im1_RT = im1_row['trial_RT'].tolist()[0]\n",
    "\n",
    "                    RTs.append(im0_RT + im1_RT)\n",
    "            \n",
    "\n",
    "        arr_VE_RT.append(np.mean(np.array(RTs)))\n",
    "        \n",
    "#     print(f\"end: {(process_time() - start_time)} seconds\")\n",
    "\n",
    "        \n",
    "    return arr_discrim_RT, arr_discrim_RT_ste, arr_VE_RT, stim_RT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_discrim_RT0_125, m_discrim_RT0_ste_125, m_VE_RT0_125, m_RT0_stim_125 = get_RT_performance(r0_125_df, VE_coded_discrim_performance0, 125) \n",
    "m_discrim_RT0_250, m_discrim_RT0_ste_250, m_VE_RT0_250, m_RT0_stim_250 = get_RT_performance(r0_250_df, VE_coded_discrim_performance0, 250) \n",
    "m_discrim_RT0_1000, m_discrim_RT0_ste_1000, m_VE_RT0_1000, m_RT0_stim_1000 = get_RT_performance(r0_1000_df, VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "m_discrim_RT1_125, m_discrim_RT1_ste_125, m_VE_RT1_125, m_RT1_stim_125 = get_RT_performance(r1_125_df, VE_coded_discrim_performance1, 125) \n",
    "m_discrim_RT1_250, m_discrim_RT1_ste_250, m_VE_RT1_250, m_RT1_stim_250 = get_RT_performance(r1_250_df, VE_coded_discrim_performance1, 250) \n",
    "m_discrim_RT1_1000, m_discrim_RT1_ste_1000, m_VE_RT1_1000, m_RT1_stim_1000 = get_RT_performance(r1_1000_df, VE_coded_discrim_performance1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7257446154004403, 8.498923006061081e-27)\n",
      "(0.7374299005982214, 4.9226136388338635e-28)\n",
      "(0.7891848088605165, 2.011512972993365e-34)\n",
      "\n",
      "(0.7761228245546422, 1.1909731396751301e-32)\n",
      "(0.6049030358552673, 6.121132413341101e-17)\n",
      "(0.6249968653143864, 2.7777152339491236e-18)\n"
     ]
    }
   ],
   "source": [
    "# STATS FOR DISCRIMINATION MATRIX FIGURE\n",
    "# accuracy between duration correlations (between splits)\n",
    "print(stats.pearsonr(m_discrim_performance0_125, m_discrim_performance1_250))\n",
    "print(stats.pearsonr(m_discrim_performance0_125, m_discrim_performance1_1000))\n",
    "print(stats.pearsonr(m_discrim_performance0_1000, m_discrim_performance1_250))\n",
    "\n",
    "print()\n",
    "\n",
    "# RT between duration correlations (between splits)\n",
    "print(stats.pearsonr(m_discrim_RT0_125, m_discrim_RT1_250))\n",
    "print(stats.pearsonr(m_discrim_RT0_125, m_discrim_RT1_1000))\n",
    "print(stats.pearsonr(m_discrim_RT0_1000, m_discrim_RT1_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6722150194727249, 7.421675223951309e-22)\n",
      "(0.3168471693270497, 5.579493474979701e-05)\n",
      "(-0.4405125212747433, 8.675609930649491e-09)\n"
     ]
    }
   ],
   "source": [
    "# STATS FOR JOINT VARIANCE MATRIX FIGURE\n",
    "print(stats.pearsonr(m_discrim_performance0_125, m_VE_performance1_125))\n",
    "print(stats.pearsonr(m_discrim_RT0_125, m_VE_RT1_250))\n",
    "print(stats.pearsonr(m_discrim_RT0_125, m_VE_performance1_1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main: Split and run analyses X num of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def permutations(nums, rows, columns):\n",
    "    M = np.zeros((rows, columns))\n",
    "    \n",
    "    for i in range(columns):\n",
    "        random.shuffle(nums)\n",
    "        for j in range(rows):\n",
    "            M[j][i] = int(nums[j])\n",
    "    \n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_scrambled(nums, rows, columns, data):\n",
    "\n",
    "    labels = list(data.keys())\n",
    "    og_data = list(data.values())\n",
    "\n",
    "    sdata = np.zeros((rows, columns))\n",
    "    \n",
    "    image_assignments = permutations(nums, rows, columns)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            sdata[i][j] = og_data[int(image_assignments[i][j])][j]\n",
    "            \n",
    "            \n",
    "    sdata_dict = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "#         print(len(sdata[idx]))\n",
    "        sdata_dict[label] = sdata[idx]\n",
    "        \n",
    "    return sdata_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, wait, as_completed\n",
    "\n",
    "\n",
    "dfs = [] \n",
    "    \n",
    "def run_analysis():\n",
    "    \n",
    "    r0_125, r0_125_df, r1_125, r1_125_df = rand_data(VE_data_125, 0.5)\n",
    "    r0_250, r0_250_df, r1_250, r1_250_df = rand_data(VE_data_250, 0.5)\n",
    "    r0_1000, r0_1000_df, r1_1000, r1_1000_df = rand_data(VE_data_1000, 0.5)\n",
    "    \n",
    "    all_discrim_performance_s0, final_discrim_s0, all_discrim_performance_s1, final_discrim_s1 = individual_discrimination_stats(FINAL_discrim, 0.5)\n",
    "    \n",
    "    ak0_125 = get_answerkey(all_discrim_performance_s0, r0_125)\n",
    "    ak0_250 = get_answerkey(all_discrim_performance_s0, r0_250)\n",
    "    ak0_1000 = get_answerkey(all_discrim_performance_s0, r0_1000)\n",
    "\n",
    "    ak1_125 = get_answerkey(all_discrim_performance_s1, r1_125)\n",
    "    ak1_250 = get_answerkey(all_discrim_performance_s1, r1_250)\n",
    "    ak1_1000 = get_answerkey(all_discrim_performance_s1, r1_1000)\n",
    "\n",
    "    VE_coded_discrim_performance0 = main_VE_coded_discrim_accuracy(final_discrim_s0, ak0_125, ak0_250, ak0_1000)\n",
    "    VE_coded_discrim_performance1 = main_VE_coded_discrim_accuracy(final_discrim_s1, ak1_125, ak1_250, ak1_1000)\n",
    "\n",
    "    discrimination_results0_125 = get_discrimination_results(VE_coded_discrim_performance0, 125)\n",
    "    discrimination_results0_250 = get_discrimination_results(VE_coded_discrim_performance0, 250)\n",
    "    discrimination_results0_1000 = get_discrimination_results(VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "    discrimination_results1_125 = get_discrimination_results(VE_coded_discrim_performance1, 125)\n",
    "    discrimination_results1_250 = get_discrimination_results(VE_coded_discrim_performance1, 250)\n",
    "    discrimination_results1_1000 = get_discrimination_results(VE_coded_discrim_performance1, 1000)\n",
    "\n",
    "    VE_performance0_125 = VE_performance(discrimination_results0_125[-1], r0_125_df, VE_coded_discrim_performance0, ak0_125)\n",
    "    VE_performance0_250 = VE_performance(discrimination_results0_250[-1], r0_250_df, VE_coded_discrim_performance0, ak0_250)\n",
    "    VE_performance0_1000 = VE_performance(discrimination_results0_1000[-1], r0_1000_df, VE_coded_discrim_performance0, ak0_1000)\n",
    "\n",
    "    VE_performance1_125 = VE_performance(discrimination_results1_125[-1], r1_125_df, VE_coded_discrim_performance1, ak1_125)\n",
    "    VE_performance1_250 = VE_performance(discrimination_results1_250[-1], r1_250_df, VE_coded_discrim_performance1, ak1_250)\n",
    "    VE_performance1_1000 = VE_performance(discrimination_results1_1000[-1], r1_1000_df, VE_coded_discrim_performance1, ak1_1000)\n",
    "\n",
    "    m_discrim_performance0_125, m_VE_performance0_125, m_stim0_125 = matched_performance(VE_performance0_125, VE_coded_discrim_performance0, 125)\n",
    "    m_discrim_performance0_250, m_VE_performance0_250, m_stim0_250 = matched_performance(VE_performance0_250, VE_coded_discrim_performance0, 250)\n",
    "    m_discrim_performance0_1000, m_VE_performance0_1000, m_stim0_1000 = matched_performance(VE_performance0_1000, VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "    m_discrim_performance1_125, m_VE_performance1_125, m_stim1_125 = matched_performance(VE_performance1_125, VE_coded_discrim_performance1, 125)\n",
    "    m_discrim_performance1_250, m_VE_performance1_250, m_stim1_250 = matched_performance(VE_performance1_250, VE_coded_discrim_performance1, 250)\n",
    "    m_discrim_performance1_1000, m_VE_performance1_1000, m_stim1_1000 = matched_performance(VE_performance1_1000, VE_coded_discrim_performance1, 1000)\n",
    "\n",
    "    m_discrim_RT0_125, m_discrim_RT0_ste_125, m_VE_RT0_125, m_RT0_stim_125 = get_RT_performance(r0_125_df, VE_coded_discrim_performance0, 125) \n",
    "    m_discrim_RT0_250, m_discrim_RT0_ste_250, m_VE_RT0_250, m_RT0_stim_250 = get_RT_performance(r0_250_df, VE_coded_discrim_performance0, 250) \n",
    "    m_discrim_RT0_1000, m_discrim_RT0_ste_1000, m_VE_RT0_1000, m_RT0_stim_1000 = get_RT_performance(r0_1000_df, VE_coded_discrim_performance0, 1000)\n",
    "\n",
    "    m_discrim_RT1_125, m_discrim_RT1_ste_125, m_VE_RT1_125, m_RT1_stim_125 = get_RT_performance(r1_125_df, VE_coded_discrim_performance1, 125) \n",
    "    m_discrim_RT1_250, m_discrim_RT1_ste_250, m_VE_RT1_250, m_RT1_stim_250 = get_RT_performance(r1_250_df, VE_coded_discrim_performance1, 250) \n",
    "    m_discrim_RT1_1000, m_discrim_RT1_ste_1000, m_VE_RT1_1000, m_RT1_stim_1000 = get_RT_performance(r1_1000_df, VE_coded_discrim_performance1, 1000)\n",
    "\n",
    "    \n",
    "    data = {'r0_Discrim_PC_125': m_discrim_performance0_125,\n",
    "        'r0_Discrim_PC_250': m_discrim_performance0_250,\n",
    "        'r0_Discrim_PC_1000': m_discrim_performance0_1000,\n",
    "        'r0_VE_PC_125': m_VE_performance0_125,\n",
    "        'r0_VE_PC_250': m_VE_performance0_250,\n",
    "        'r0_VE_PC_1000': m_VE_performance0_1000,\n",
    "        'r0_Discrim 125 RT': m_discrim_RT0_125,\n",
    "        'r0_Discrim 250 RT': m_discrim_RT0_250,\n",
    "        'r0_Discrim 1000 RT': m_discrim_RT0_1000,\n",
    "        'r0_VE 125 RT': m_VE_RT0_125,\n",
    "        'r0_VE 250 RT': m_VE_RT0_250,\n",
    "        'r0_VE 1000 RT': m_VE_RT0_1000,  \n",
    "        'r1_Discrim_PC_125': m_discrim_performance1_125,\n",
    "        'r1_Discrim_PC_250': m_discrim_performance1_250,\n",
    "        'r1_Discrim_PC_1000': m_discrim_performance1_1000,\n",
    "        'r1_VE_PC_125': m_VE_performance1_125,\n",
    "        'r1_VE_PC_250': m_VE_performance1_250,\n",
    "        'r1_VE_PC_1000': m_VE_performance1_1000,\n",
    "        'r1_Discrim 125 RT': m_discrim_RT1_125,\n",
    "        'r1_Discrim 250 RT': m_discrim_RT1_250,\n",
    "        'r1_Discrim 1000 RT': m_discrim_RT1_1000,\n",
    "        'r1_VE 125 RT': m_VE_RT1_125,\n",
    "        'r1_VE 250 RT': m_VE_RT1_250,\n",
    "        'r1_VE 1000 RT': m_VE_RT1_1000\n",
    "        }\n",
    "    \n",
    "    \n",
    "    data_IES = {'r0_discrim_IES_125': np.array(m_discrim_RT0_125)/np.array(m_discrim_performance0_125),\n",
    "                'r0_discrim_IES_250': np.array(m_discrim_RT0_250)/np.array(m_discrim_performance0_250),\n",
    "                'r0_discrim_IES_1000': np.array(m_discrim_RT0_1000)/np.array(m_discrim_performance0_1000),\n",
    "                'r1_discrim_IES_125': np.array(m_discrim_RT1_125)/np.array(m_discrim_performance1_125),\n",
    "                'r1_discrim_IES_250': np.array(m_discrim_RT1_250)/m_discrim_performance1_250,\n",
    "                'r1_discrim_IES_1000': np.array(m_discrim_RT1_1000)/np.array(m_discrim_performance1_1000),\n",
    "                'r0_VE_IES_125': np.array(m_VE_RT0_125)/np.array(m_VE_performance0_125),\n",
    "                'r0_VE_IES_250': np.array(m_VE_RT0_250)/np.array(m_VE_performance0_250),\n",
    "                'r0_VE_IES_1000': np.array(m_VE_RT0_1000)/np.array(m_VE_performance0_1000),\n",
    "                'r1_VE_IES_125': np.array(m_VE_RT1_125)/np.array(m_VE_performance1_125),\n",
    "                'r1_VE_IES_250': np.array(m_VE_RT1_250)/np.array(m_VE_performance1_250),\n",
    "                'r1_VE_IES_1000': np.array(m_VE_RT1_1000)/np.array(m_VE_performance1_1000)}\n",
    "    \n",
    "    # 50% random mixing \n",
    "    # one instance of each image in each vector \n",
    "    \n",
    "    #r0\n",
    "    r0_125_og = {'r0_discrim_IES_125': np.array(m_discrim_RT0_125)/np.array(m_discrim_performance0_125),\n",
    "                 'r0_VE_IES_125': np.array(m_VE_RT0_125)/np.array(m_VE_performance0_125)}\n",
    "\n",
    "    r0_125_scrambled = task_scrambled([0,1], 2, 156, r0_125_og)\n",
    "    \n",
    "    r0_250_og = {'r0_discrim_IES_250': np.array(m_discrim_RT0_250)/np.array(m_discrim_performance0_250),\n",
    "                 'r0_VE_IES_250': np.array(m_VE_RT0_250)/np.array(m_VE_performance0_250)}\n",
    "\n",
    "    r0_250_scrambled = task_scrambled([0,1], 2, 156, r0_250_og)\n",
    "    \n",
    "    r0_1000_og = {'r0_discrim_IES_1000': np.array(m_discrim_RT0_1000)/np.array(m_discrim_performance0_1000),\n",
    "                 'r0_VE_IES_1000': np.array(m_VE_RT0_1000)/np.array(m_VE_performance0_1000)}\n",
    "\n",
    "    r0_1000_scrambled = task_scrambled([0,1], 2, 156, r0_1000_og)\n",
    "    \n",
    "    \n",
    "    #r1\n",
    "    r1_125_og = {'r1_discrim_IES_125': np.array(m_discrim_RT1_125)/np.array(m_discrim_performance1_125),\n",
    "                 'r1_VE_IES_125': np.array(m_VE_RT1_125)/np.array(m_VE_performance1_125)}\n",
    "\n",
    "    r1_125_scrambled = task_scrambled([0,1], 2, 156, r1_125_og)\n",
    "    \n",
    "    r1_250_og = {'r1_discrim_IES_250': np.array(m_discrim_RT1_250)/np.array(m_discrim_performance1_250),\n",
    "                 'r1_VE_IES_250': np.array(m_VE_RT1_250)/np.array(m_VE_performance1_250)}\n",
    "\n",
    "    r1_250_scrambled = task_scrambled([0,1], 2, 156, r1_250_og)\n",
    "    \n",
    "    r1_1000_og = {'r1_discrim_IES_1000': np.array(m_discrim_RT1_1000)/np.array(m_discrim_performance1_1000),\n",
    "                  'r1_VE_IES_1000': np.array(m_VE_RT1_1000)/np.array(m_VE_performance1_1000)}\n",
    "\n",
    "    r1_1000_scrambled = task_scrambled([0,1], 2, 156, r1_1000_og)\n",
    "\n",
    "\n",
    "\n",
    "    # combine the scrambled data back together\n",
    "    scrambled_IES = {**r0_125_scrambled, **r0_250_scrambled, **r0_1000_scrambled, **r1_125_scrambled, **r1_250_scrambled, **r1_1000_scrambled}\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(scrambled_IES,columns=['r0_discrim_IES_125', 'r0_discrim_IES_250', 'r0_discrim_IES_1000', \n",
    "                                             'r1_discrim_IES_125', 'r1_discrim_IES_250', 'r1_discrim_IES_1000',\n",
    "                                             'r0_VE_IES_125', 'r0_VE_IES_250', 'r0_VE_IES_1000', \n",
    "                                             'r1_VE_IES_125', 'r1_VE_IES_250', 'r1_VE_IES_1000'])\n",
    "\n",
    "    df_corr = df.corr()\n",
    "    \n",
    "    return df_corr    \n",
    "\n",
    "# save each correlation matrix: INSERT YOUR PATH HERE\n",
    "p = ''\n",
    "\n",
    "\n",
    "reps = 2\n",
    "dfs = []\n",
    "import multiprocessing as mp\n",
    "\n",
    "executor = ProcessPoolExecutor(max_workers=mp.cpu_count()//4)\n",
    "for i in range(reps):\n",
    "    print(\"Enqueuing run_analysis rep: \", i)\n",
    "    dfs.append(executor.submit(run_analysis))\n",
    "\n",
    "for count, future in enumerate(as_completed(dfs)):\n",
    "    print(\"Rep completed :\", count)\n",
    "    dest = p + 'ies-' + str(count) + '.pkl'\n",
    "    dfs[count].result().to_pickle(dest)\n",
    "    \n",
    "df_sum = 0\n",
    "for i, df in enumerate(dfs):\n",
    "    df_sum += df.result()\n",
    "\n",
    "\n",
    "mean_df_corr = df_sum / len(dfs)\n",
    "executor.shutdown(wait=True)\n",
    "#### INSERT YOUR PATH HERE ####\n",
    "d = ''\n",
    "mean_df_corr.to_csv(d + 'avg_taskScrambledIES_randSplitMatrix_n' + str(reps) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)\n",
    "# dfs[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_mean_df_corr = mean_df_corr.tail(6)\n",
    "bottom_mean_df_corr = bottom_mean_df_corr.drop(columns = ['r1_VE_IES_1000', 'r1_VE_IES_250', 'r1_VE_IES_125', 'r0_VE_IES_1000', 'r0_VE_IES_250', 'r0_VE_IES_125'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,25])\n",
    "\n",
    "import matplotlib.colors\n",
    "\n",
    "norm = matplotlib.colors.Normalize(-1,1)\n",
    "colors = [[norm(-1.0), \"aqua\"],\n",
    "          [norm(-0.2), \"blue\"],\n",
    "          [norm(0), \"black\"],\n",
    "          [norm( 0.2), \"red\"],\n",
    "          [norm( 1.0), \"yellow\"]]\n",
    "\n",
    "\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "\n",
    "ax = sns.heatmap(mean_df_corr, annot=True, fmt=\".2f\", cmap= cmap, square=True, center=0, vmin=-1, vmax=1, edgecolors='w', linewidths=3, cbar_kws={\"shrink\": .82}, ) #notation: \"annot\" not \"annote\"\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "\n",
    "# plt.savefig('taskScrambled_IES_5000.png')\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,20])\n",
    "\n",
    "ax = sns.heatmap(bottom_mean_df_corr, annot=True, fmt=\".2f\", cmap= cmap, square=True, center=0, vmin=-1, vmax=1, edgecolors='w', linewidths=3, cbar_kws={\"shrink\": .82}, annot_kws={'fontsize': 22, 'color': 'white'}) #notation: \"annot\" not \"annote\"\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "\n",
    "# plt.savefig('taskScrambled_IES_bottom_rand_' + str(reps) + '.png')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
